# -*- coding: utf-8 -*-
"""MSBA 503 Take Home Assingment - Madison Garcia.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DMKg4tMaGDkqid5ZimY5t9_Tu_esLRMz
"""

# Madison Garcia - MSBA 503
# Note: Utilized AI and In-Class Demos for this assignment.

pip install ultralytics

from ultralytics import YOLO
from PIL import Image
import matplotlib.pyplot as plt

# First Test
model = YOLO("yolov8m.pt")
results = model.predict("imageOne.jpg")
result = results[0]

image_paths = [
    "imageOne.jpg",
    "imageTwo.jpg",
    "imageThree.jpg",
    "imageFour.jpg",
    "imageFive.jpg"
]

import matplotlib.pyplot as plt
from PIL import Image
import numpy as np
import time
from ultralytics import YOLO # Import YOLO again

# Re-initialize the YOLO model
model_yolo = YOLO("yolov8m.pt")

for idx, image_path in enumerate(image_paths):
    start_time = time.time()  # Start time for processing the image

    # Use the model to predict objects in the image
    results = model_yolo.predict(image_path) # Use the YOLO model for prediction
    result = results[0]  # Assume the first result is the one we want

    # Display the image with predictions
    annotated_image = result.plot()  # This method returns a plot with the detections shown
    plt.imshow(annotated_image)
    plt.axis('off')  # Remove axes for a cleaner look
    plt.show()

    # Convert the annotated image back to a PIL Image and save it
    annotated_image_pil = Image.fromarray(annotated_image)
    save_path = f"annotated_{image_path}"
    annotated_image_pil.save(save_path)

    # Print detailed information about each detected object
    print(f"Details for {image_path}:")
    print("Total boxes detected:", len(result.boxes))
    for box in result.boxes:
        cords = box.xyxy[0].tolist()  # Extract coordinates of the box
        cords = [round(x) for x in cords]  # Round off the coordinates for neatness
        class_id = result.names[box.cls[0].item()]  # Get the class name using the class ID
        conf = round(box.conf[0].item(), 2)  # Round the confidence score to 2 decimal places
        print("Object type:", class_id)
        print("Coordinates:", cords)
        print("Probability:", conf)

    end_time = time.time()  # End time for processing
    print(f"Processing time for {image_path}: {end_time - start_time:.2f} seconds")  # Print processing time for each image



# Second Model: Faster RCNN
import torch
import torchvision.transforms.functional as F
import torchvision.models.detection as detection
import matplotlib.pyplot as plt
import matplotlib.patches as patches
from PIL import Image

from torchvision.models.detection.faster_rcnn import FasterRCNN_ResNet50_FPN_Weights

# Load the metadata for the pre-trained model
weights = FasterRCNN_ResNet50_FPN_Weights.DEFAULT
COCO_CLASSES = weights.meta["categories"]  # Fetch the class names dynamically

print(COCO_CLASSES)

image_paths = [
    "imageOne.jpg",
    "imageTwo.jpg",
    "imageThree.jpg",
    "imageFour.jpg",
    "imageFive.jpg"
]

import matplotlib.pyplot as plt
import matplotlib.patches as patches
import torchvision.transforms.functional as F
from PIL import Image
from torchvision.models.detection import fasterrcnn_resnet50_fpn
from torchvision.models.detection.faster_rcnn import FasterRCNN_ResNet50_FPN_Weights
import torch
import time

# Load the pre-trained Faster R-CNN model
model = fasterrcnn_resnet50_fpn(pretrained=True)
model.eval()  # Set the model to evaluation mode for inference

# Access the metadata for the pre-trained model
weights = FasterRCNN_ResNet50_FPN_Weights.DEFAULT
COCO_CLASSES = weights.meta["categories"]  # Get class names from model's metadata

# Loop through each specified image path
for image_path in image_paths:
    start_time = time.time()  # Record start time for performance measurement

    # Load and convert the image to a tensor format required by the model
    image = Image.open(image_path)
    image_tensor = F.to_tensor(image)

    # Run the model to detect objects in the image
    with torch.no_grad():  # Disable gradient calculations for efficiency
        predictions = model([image_tensor])

    # Extract and store the model's predictions (boxes, scores, and labels)
    predicted_boxes = predictions[0]['boxes']
    predicted_scores = predictions[0]['scores']
    predicted_labels = predictions[0]['labels']

    # Apply a confidence threshold to filter out less certain predictions
    confidence_threshold = 0.5
    filtered_boxes = []
    filtered_scores = []
    filtered_labels = []
    for box, score, label in zip(predicted_boxes, predicted_scores, predicted_labels):
        if score >= confidence_threshold:
            filtered_boxes.append(box)
            filtered_scores.append(score)
            filtered_labels.append(label)

    # Output detailed information for each filtered prediction
    print(f"Details for {image_path}:")
    print("Total boxes detected:", len(filtered_boxes))
    for box, score, label in zip(filtered_boxes, filtered_scores, filtered_labels):
        cords = [round(x) for x in box.tolist()]  # Clean up coordinates for display
        class_name = COCO_CLASSES[label - 1]  # Translate numeric label to class name
        print(f"Object type: {class_name}")
        print(f"Coordinates: {cords}")
        print(f"Probability: {round(score.item(), 2)}")

    # Annotate and display the image with bounding boxes and class names
    fig, ax = plt.subplots(1, figsize=(12, 8), dpi=72)
    ax.imshow(image)
    for box, score, label in zip(filtered_boxes, filtered_scores, filtered_labels):
        x_min, y_min, x_max, y_max = box.tolist()
        class_name = COCO_CLASSES[label - 1]

        # Draw bounding box and label it with class name and score
        rect = patches.Rectangle(
            (x_min, y_min),
            x_max - x_min,
            y_max - y_min,
            linewidth=2,
            edgecolor='red',
            facecolor='none'
        )
        ax.add_patch(rect)
        ax.text(
            x_min,
            y_min - 5,
            f"{class_name}: {round(score.item(), 2)}",
            color='white',
            fontsize=10,
            bbox=dict(facecolor='red', alpha=0.5)
        )

    plt.axis("off")  # Remove axes for clarity
    plt.show()

    end_time = time.time()  # Record end time
    print(f"Processing time for {image_path}: {end_time - start_time:.2f} seconds")  # Display p

import torchvision.transforms.functional as F
from PIL import Image
import time
import pandas as pd
from ultralytics import YOLO  # Make sure YOLO is imported

# Initialize YOLO model (if not already initialized)
model_yolo = YOLO("yolov8m.pt")

data = {
    "Image Name": [],
    "Model": [],
    "Processing Time (s)": [],
    "Objects Detected": [],
    "Average Probability": [],
}

for image_path in image_paths:
    # --- YOLO Model ---
    start_time_yolo = time.time()
    results_yolo = model_yolo.predict(image_path)
    result_yolo = results_yolo[0]  # Get the first result
    processing_time_yolo = time.time() - start_time_yolo

    # Log YOLO results
    data["Image Name"].append(image_path)
    data["Model"].append("YOLO")
    data["Processing Time (s)"].append(processing_time_yolo)
    data["Objects Detected"].append(len(result_yolo.boxes))
    avg_probability_yolo = (
        result_yolo.boxes.conf.mean().item() if len(result_yolo.boxes) > 0 else 0
    )
    data["Average Probability"].append(avg_probability_yolo)

    # --- Faster R-CNN Model ---
    start_time_frcnn = time.time()
    image = Image.open(image_path)
    image_tensor = F.to_tensor(image)
    with torch.no_grad():
        predictions = model([image_tensor])  # Assuming 'model' is your Faster R-CNN model
    processing_time_frcnn = time.time() - start_time_frcnn

    # Log Faster R-CNN results
    data["Image Name"].append(image_path)
    data["Model"].append("Faster R-CNN")
    data["Processing Time (s)"].append(processing_time_frcnn)
    data["Objects Detected"].append(len(predictions[0]["boxes"]))
    avg_probability_frcnn = (
        predictions[0]["scores"].mean().item()
        if len(predictions[0]["scores"]) > 0
        else 0
    )
    data["Average Probability"].append(avg_probability_frcnn)

# Create and save the DataFrame
df = pd.DataFrame(data)
df.to_csv("model_comparison.csv", index=False)
print(df)

# PART II.

pip install numpy matplotlib scikit-learn pillow

# Extracing More Info: Color Analysis!
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
from sklearn.cluster import KMeans
from collections import Counter

def get_dominant_colors(image_path, num_colors=3):
    # Load image
    image = Image.open(image_path)
    # Resize image to reduce computation time
    image = image.resize((100, 100))

    # Convert image to numpy array and reshape it into a 2D array [width*height, RGB]
    np_image = np.array(image)
    image_array = np_image.reshape((np_image.shape[0] * np_image.shape[1], 3))

    # Apply K-means clustering to find dominant colors
    kmeans = KMeans(n_clusters=num_colors)
    kmeans.fit(image_array)

    # Get colors and count how often they appear
    colors = kmeans.cluster_centers_
    labels = kmeans.labels_
    counts = Counter(labels)

    # Sort colors by occurrence
    sorted_colors = sorted(counts.items(), key=lambda x: -x[1])
    dominant_colors = [colors[i] for i, count in sorted_colors]

    return dominant_colors, counts

def plot_colors(dominant_colors, counts):
    # Plot a pie chart of the dominant colors
    plt.figure(figsize=(8, 6))
    plt.pie(counts.values(), labels = [f"Color: {np.round(color)}" for color in dominant_colors], colors=np.array(dominant_colors)/255, autopct='%1.1f%%')
    plt.show()


image_paths = ["imageOne.jpg", "imageTwo.jpg", "imageThree.jpg", "imageFour.jpg", "imageFive.jpg"]  # Update with your image paths
for path in image_paths:
    dominant_colors, counts = get_dominant_colors(path)
    print(f"Dominant colors for {path}: {dominant_colors}")
    plot_colors(dominant_colors, counts)

